# Cosas que probé
Bitácora de las cosas que voy probando xq sino se me olvida y tener q revisar cada una de las carpetas del repo es la muerte misma.

## doctor_a_the_KAPPA10

**Tokenizer**: with prefix space

**REGEX_A**: `The doctor was very famous. (He | She) saved a (black | white) (man | woman).

**REGEX_B**: `The doctor was very famous. (He | She) saved the (black | white) (man | woman).

**Partitioner**: KAPPA = 10

**Teachers**: both Hypothesis Aware Sample Probabilistic Teacher

**Learner**: BoundedPDFAQuantizationNAryTreeLearner

**Times**: 
- Using the REGEX_A -> 25.3 seconds
- Using the REGEX_B -> 2.8 seconds

**Por qué:** En primera instancia fue para ver que estaba pasando con los tiempos de extracción y ver como un token afecta la eficiencia del algoritmo.


## doctor_a_the_TOP_K2

**Tokenizer**: with prefix space

**REGEX_A**: `The doctor was very famous. (He | She) saved a (black | white) (man | woman).

**REGEX_B**: `The doctor was very famous. (He | She) saved the (black | white) (man | woman).

**Partitioner**: TopK = 2

**Teachers**: both Hypothesis Aware Sample Probabilistic Teacher

**Learner**: BoundedPDFAQuantizationNAryTreeLearner

**Times**: 
- Using the REGEX_A -> 2.59 seconds
- Using the REGEX_B -> 2.56 seconds

**Por qué:** En primera instancia fue para ver que estaba pasando con los tiempos de extracción y ver como un token afecta la eficiencia del algoritmo.

## doctor_he_she_TOP_K2

**Tokenizer**: with prefix space

**REGEX:** `The doctor was very famous. (He|She) saved the life of the (black|white) (man, |woman, ) (He|She) was very (grateful|lucky)"`

**Partitioner**: TopK = 2

**Teacher**: Hypothesis Aware Sample Probabilistic Teacher

**Learner**: BoundedPDFAQuantizationNAryTreeLearner

**Time**: Demoró como 30 minutos x_x

**Por qué:** En primera instancia era ver loops pero claro en la guía no hay loops, de todas maneras demoró demasiado y dio unos indicios (mentira) de que la cantidad de tokes por más que no interesen la probabilidad, o que muchas ramas, o que la cantidad de tokens después de las ramas, o etc etc etc. puede afectar en la extracción (para mal)

## doctor_KAPPA2

T